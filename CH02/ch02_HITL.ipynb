{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Open in Colab\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/Nicolepcx/ai-agents-the-definitive-guide/blob/main/CH02/ch02_HITL.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n"
      ],
      "metadata": {
        "id": "pWwE02-c7-ON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About this notebook\n",
        "\n",
        "This notebook is a hands-on tour of Human-in-the-Loop patterns for LangGraph. It shows how to add precise human control to agent workflows, from lightweight approval gates to full interactive editing, while keeping the code compact and production minded.\n",
        "\n",
        "## What you will learn\n",
        "\n",
        "1. How to wire LangGraph interrupts to pause a run, collect human input, and resume deterministically.\n",
        "2. How to checkpoint state with `InMemorySaver` so a run can stop and continue without losing context.\n",
        "3. How to wrap tools so a human can accept, edit, or override a call before it executes.\n",
        "4. How to drive simple ReAct style loops that honor human review of tool calls.\n",
        "5. How to implement parallel interrupts and resume them with a single map.\n",
        "\n",
        "## Models and configuration\n",
        "\n",
        "The notebook auto selects the model at runtime.\n",
        "\n",
        "* If `NEBIUS_API_KEY` is present, it calls `meta-llama/Llama-3.3-70B-Instruct-fast` through the Nebius endpoint using `ChatOpenAI` with a custom `base_url`.\n",
        "* Otherwise it defaults to `gpt-4o-mini` through `langchain_openai`.\n",
        "\n",
        "Environment variables are loaded from `.env` via `python-dotenv`:\n",
        "\n",
        "* `OPENAI_API_KEY` for OpenAI models\n",
        "* `NEBIUS_API_KEY` for Nebius Studio\n",
        "\n",
        "## The patterns showcased\n",
        "\n",
        "**Pattern A — Human feedback loop on content generation**\n",
        "A simple write–review loop for LinkedIn posts. The model drafts. The human provides iterative feedback via `interrupt`. The graph loops until the human types done. Useful for any short form content workflow that benefits from iterative refinement.\n",
        "\n",
        "**Pattern B — Approval gate before a critical call**\n",
        "The model proposes an HTTP request as JSON. A human inspects and can approve or revise before the code performs the external call. This is a minimal but powerful safety interlock for network or finance critical actions.\n",
        "\n",
        "**Pattern C — Review and edit state**\n",
        "The model writes a short summary. The human edits the text in place. The edited text becomes the new state. This pattern is ideal for compliance or brand voice checks.\n",
        "\n",
        "**Pattern D — Parallel interrupts with a single resume map**\n",
        "Two independent interrupts fire at once. The runner prints both payloads and collects a resume value for each, then resumes the graph in one step. This is a template for multi item review tasks.\n",
        "\n",
        "**Pattern E — Tool call review inside a tiny ReAct loop**\n",
        "A tool is wrapped with `add_hitl`. Before execution, a human can accept, edit arguments, or respond with a stub result. The loop continues until the model no longer requests tools. This is the smallest possible example of supervised tool use.\n",
        "\n",
        "**Pattern F — Static interrupts for debugging**\n",
        "Graph level interrupts are registered before and after specific nodes to create deterministic breakpoints. This is useful for stepwise debugging and unit style tests.\n",
        "\n",
        "## How the interactive runner works\n",
        "\n",
        "* Each demo builds a compiled graph and starts a new thread id for clean state.\n",
        "* When an interrupt occurs, the terminal prints the payload and waits for input.\n",
        "* You can paste raw strings or JSON. For the tool wrapper in Pattern E you can provide:\n",
        "\n",
        "  * `{\"type\": \"accept\"}`\n",
        "  * `{\"type\": \"edit\", \"args\": {\"args\": {\"query\": \"weather in Zurich\"}}}`\n",
        "  * `{\"type\": \"response\", \"args\": \"Skip for now\"}`\n",
        "\n",
        "For parallel interrupts the runner prints a numbered list and collects a resume value per interrupt id, then resumes once with a single `Command(resume=...)`.\n",
        "\n",
        "## Dependencies\n",
        "\n",
        "* `langgraph` for graphs, interrupts, checkpoints, and tasks\n",
        "* `langchain_openai` for LLM bindings\n",
        "* `python-dotenv` for environment loading\n",
        "* `requests` used in Pattern B for a real HTTP GET\n",
        "\n",
        "## How to run\n",
        "\n",
        "1. Create a `.env` file with your keys. At least one of `OPENAI_API_KEY` or `NEBIUS_API_KEY` must be set.\n",
        "2. Run the notebook and choose a demo from the menu printed by `main()`.\n",
        "3. Follow the terminal prompts to provide feedback or approvals.\n",
        "\n",
        "## Why this matters\n",
        "\n",
        "Real systems in finance, research, and operations often need both autonomy and control. These patterns show how to add precise human control without fighting the agent architecture. Each pattern composes cleanly, so you can start small, measure impact, and expand to richer supervision where it adds the most value.\n"
      ],
      "metadata": {
        "id": "iND9cS50E4A2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langgraph==0.6.7 langchain-openai==0.3.33 python-dotenv==1.1.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acaz6QH582FE",
        "outputId": "d2bf0a54-6835-40e0-e268-90cea727024e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- API Key Setup ---\n",
        "# Option 1 (preferred): create a `.env` file in your project folder with:\n",
        "# OPENAI_API_KEY=your_openai_key_here\n",
        "# SERPAPI_API_KEY=your_serpapi_key_here\n",
        "#\n",
        "# Option 2: set it directly in the notebook with magic:\n",
        "# %env OPENAI_API_KEY=your_openai_key_here\n",
        "# %env SERPAPI_API_KEY=your_serpapi_key_here\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load from .env if available\n",
        "load_dotenv()\n",
        "\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "serp_api_key = os.getenv(\"SERPAPI_API_KEY\")\n",
        "NEBIUS_API_KEY = os.getenv(\"NEBIUS_API_KEY\")\n",
        "\n",
        "# Fallback: ask if still missing\n",
        "if not OPENAI_API_KEY:\n",
        "    print(\"⚠️ OPENAI_API_KEY not found. You can set it with `%env` in the notebook or enter it below.\")\n",
        "    OPENAI_API_KEY = input(\"Enter your OPENAI_API_KEY: \").strip()\n",
        "\n",
        "if not serp_api_key:\n",
        "    print(\"⚠️ SERPAPI_API_KEY not found. You can set it with `%env` in the notebook or enter it below.\")\n",
        "    serp_api_key = input(\"Enter your SERPAPI_API_KEY: \").strip()\n",
        "\n",
        "if not NEBIUS_API_KEY:\n",
        "    print(\"⚠️ NEBIUS_API_KEY not found. You can set it with `%env` in the notebook or enter it below.\")\n",
        "    serp_api_key = input(\"Enter your NEBIUS_API_KEY: \").strip()\n",
        "\n",
        "print(\"✅ API keys loaded successfully!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "jVb7DLioAGyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "4MMDGc0vRLBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import os, uuid, json, re, sys\n",
        "from typing import Any, Dict, List, Optional, TypedDict, Literal\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.constants import START, END\n",
        "from langgraph.types import interrupt, Command\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "from langgraph.func import entrypoint, task\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import ToolMessage\n",
        "from langchain_core.tools import tool, BaseTool\n"
      ],
      "metadata": {
        "id": "U_afSJy_RKLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM setup (Nebius if available, else OpenAI)"
      ],
      "metadata": {
        "id": "0zg6Xfk_RSJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NEBIUS_API_KEY = os.getenv(\"NEBIUS_API_KEY\")\n",
        "if NEBIUS_API_KEY:\n",
        "    LLM = ChatOpenAI(\n",
        "        model=\"meta-llama/Llama-3.3-70B-Instruct-fast\",\n",
        "        temperature=0,\n",
        "        max_tokens=None,\n",
        "        timeout=None,\n",
        "        max_retries=2,\n",
        "        api_key=NEBIUS_API_KEY,\n",
        "        base_url=\"https://api.studio.nebius.ai/v1/\",\n",
        "    )\n",
        "else:\n",
        "    LLM = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "CHECKPOINTER = InMemorySaver()\n",
        "\n",
        "def jdump(x):\n",
        "    try:\n",
        "        return json.dumps(x, indent=2, ensure_ascii=False, default=str)\n",
        "    except Exception:\n",
        "        return str(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "pgqlZkr_RQ07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pattern A: Human feedback loop on content generation"
      ],
      "metadata": {
        "id": "Wi7zoCHtRVVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AState(TypedDict, total=False):\n",
        "    linkedin_topic: str\n",
        "    generated_post: str\n",
        "    human_feedback: List[str]\n",
        "\n",
        "def a_model(state: AState) -> AState:\n",
        "    topic = state[\"linkedin_topic\"]\n",
        "    fb = state.get(\"human_feedback\", [])\n",
        "    prompt = f\"\"\"\n",
        "LinkedIn Topic: {topic}\n",
        "Most recent human feedback: {fb[-1] if fb else \"No feedback yet\"}\n",
        "\n",
        "Write a concise LinkedIn post. Consider feedback if present.\n",
        "\"\"\"\n",
        "    resp = LLM.invoke(prompt).content\n",
        "    print(\"\\n[model] Draft:\\n\" + resp + \"\\n\")\n",
        "    return {\"generated_post\": resp, \"human_feedback\": fb}\n",
        "\n",
        "def a_human(state: AState):\n",
        "    print(\"\\n[human] awaiting feedback. Type done to finish\")\n",
        "    payload = {\n",
        "        \"generated_post\": state[\"generated_post\"],\n",
        "        \"message\": \"Provide feedback or type done\"\n",
        "    }\n",
        "    feedback = interrupt(payload)\n",
        "    print(\"[human] feedback:\", feedback)\n",
        "    if isinstance(feedback, str) and feedback.strip().lower() in {\"done\", \"quit\", \"exit\"}:\n",
        "        return Command(goto=\"a_end\", update={\"human_feedback\": state.get(\"human_feedback\", []) + [\"Finalised\"]})\n",
        "    return Command(goto=\"a_model\", update={\"human_feedback\": state.get(\"human_feedback\", []) + [str(feedback)]})\n",
        "\n",
        "def a_end(state: AState) -> AState:\n",
        "    print(\"\\n[end] Final post:\\n\" + state[\"generated_post\"])\n",
        "    print(\"[end] Feedback trail:\", state.get(\"human_feedback\", []))\n",
        "    return state\n",
        "\n",
        "def build_graph_A():\n",
        "    g = StateGraph(AState)\n",
        "    g.add_node(\"a_model\", a_model)\n",
        "    g.add_node(\"a_human\", a_human)\n",
        "    g.add_node(\"a_end\", a_end)\n",
        "    g.set_entry_point(\"a_model\")\n",
        "    g.add_edge(\"a_model\", \"a_human\")\n",
        "    g.add_edge(\"a_end\", END)\n",
        "    return g.compile(checkpointer=CHECKPOINTER)\n"
      ],
      "metadata": {
        "id": "PdOzFnaCRV2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pattern B: Approval gate before critical call"
      ],
      "metadata": {
        "id": "tsZls1FpRZgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BState(TypedDict, total=False):\n",
        "    proposed_request: Dict[str, Any]\n",
        "    api_result: Dict[str, Any]\n",
        "    decision: str\n",
        "\n",
        "def b_propose(state: BState) -> BState:\n",
        "    prompt = \"Return only JSON with keys url and params for GET to https://httpbin.org/get using q and limit.\"\n",
        "    text = LLM.invoke(prompt).content\n",
        "    m = re.search(r\"\\{.*\\}\", text, re.S)\n",
        "    data = {\"url\": \"https://httpbin.org/get\", \"params\": {\"q\": \"fallback\", \"limit\": 1}}\n",
        "    if m:\n",
        "        try:\n",
        "            data = json.loads(m.group(0))\n",
        "        except Exception:\n",
        "            pass\n",
        "    return {\"proposed_request\": data}\n",
        "\n",
        "def b_gate(state: BState) -> Command[Literal[\"b_call\", \"b_propose\"]]:\n",
        "    v = interrupt({\n",
        "        \"question\": \"Approve or revise request\",\n",
        "        \"proposed_request\": state[\"proposed_request\"],\n",
        "        \"schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"action\": {\"enum\": [\"approve\", \"revise\"]},\n",
        "                \"update\": {\"type\": \"object\"}\n",
        "            },\n",
        "            \"required\": [\"action\"]\n",
        "        }\n",
        "    })\n",
        "    action = v.get(\"action\")\n",
        "    if action == \"approve\":\n",
        "        return Command(goto=\"b_call\", update={\"decision\": \"approved\"})\n",
        "    upd = v.get(\"update\") or {}\n",
        "    new_req = state[\"proposed_request\"].copy()\n",
        "    if \"url\" in upd:\n",
        "        new_req[\"url\"] = upd[\"url\"]\n",
        "    if isinstance(upd.get(\"params\"), dict):\n",
        "        new_req.setdefault(\"params\", {}).update(upd[\"params\"])\n",
        "    return Command(goto=\"b_gate\", update={\"proposed_request\": new_req, \"decision\": \"revised\"})\n",
        "\n",
        "def b_call(state: BState) -> BState:\n",
        "    import requests\n",
        "    r = requests.get(state[\"proposed_request\"][\"url\"], params=state[\"proposed_request\"][\"params\"], timeout=10)\n",
        "    return {\"api_result\": {\"status_code\": r.status_code, \"url\": r.url}}\n",
        "\n",
        "def build_graph_B():\n",
        "    g = StateGraph(BState)\n",
        "    g.add_node(\"b_propose\", b_propose)\n",
        "    g.add_node(\"b_gate\", b_gate)\n",
        "    g.add_node(\"b_call\", b_call)\n",
        "    g.set_entry_point(\"b_propose\")\n",
        "    g.add_edge(\"b_propose\", \"b_gate\")\n",
        "    g.add_edge(\"b_gate\", \"b_call\")\n",
        "    g.add_edge(\"b_call\", END)\n",
        "    return g.compile(checkpointer=CHECKPOINTER)"
      ],
      "metadata": {
        "id": "J3B_ZK6eRaHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pattern C: Review and edit state"
      ],
      "metadata": {
        "id": "knjNOUZtReCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CState(TypedDict, total=False):\n",
        "    summary: str\n",
        "\n",
        "def c_write(state: CState) -> CState:\n",
        "    text = LLM.invoke(\"Write 2 sentences about why human in the loop matters for agents\").content\n",
        "    return {\"summary\": text}\n",
        "\n",
        "def c_edit(state: CState) -> CState:\n",
        "    res = interrupt({\n",
        "        \"task\": \"Edit the summary text\",\n",
        "        \"summary\": state[\"summary\"],\n",
        "        \"schema\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\"edited_text\": {\"type\": \"string\"}},\n",
        "            \"required\": [\"edited_text\"]\n",
        "        }\n",
        "    })\n",
        "    return {\"summary\": res[\"edited_text\"]}\n",
        "\n",
        "def build_graph_C():\n",
        "    g = StateGraph(CState)\n",
        "    g.add_node(\"c_write\", c_write)\n",
        "    g.add_node(\"c_edit\", c_edit)\n",
        "    g.set_entry_point(\"c_write\")\n",
        "    g.add_edge(\"c_write\", \"c_edit\")\n",
        "    g.add_edge(\"c_edit\", END)\n",
        "    return g.compile(checkpointer=CHECKPOINTER)"
      ],
      "metadata": {
        "id": "HUSyKdF4Re1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pattern D: Parallel interrupts with single resume map"
      ],
      "metadata": {
        "id": "VJnheKx_Ri-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DState(TypedDict, total=False):\n",
        "    text_1: str\n",
        "    text_2: str\n",
        "\n",
        "def d_h1(state: DState):\n",
        "    v = interrupt({\"text_to_revise\": state[\"text_1\"]})\n",
        "    return {\"text_1\": v}\n",
        "\n",
        "def d_h2(state: DState):\n",
        "    v = interrupt({\"text_to_revise\": state[\"text_2\"]})\n",
        "    return {\"text_2\": v}\n",
        "\n",
        "def build_graph_D():\n",
        "    g = StateGraph(DState)\n",
        "    g.add_node(\"d_h1\", d_h1)\n",
        "    g.add_node(\"d_h2\", d_h2)\n",
        "    g.add_edge(START, \"d_h1\")\n",
        "    g.add_edge(START, \"d_h2\")\n",
        "    g.add_edge(\"d_h1\", END)\n",
        "    g.add_edge(\"d_h2\", END)\n",
        "    return g.compile(checkpointer=CHECKPOINTER)"
      ],
      "metadata": {
        "id": "_kK6DfPJRlX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pattern E: Tool call review in a tiny ReAct loop"
      ],
      "metadata": {
        "id": "P-Y-oQEZRnI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_hitl(tool_obj: BaseTool | Any) -> BaseTool:\n",
        "    if not isinstance(tool_obj, BaseTool):\n",
        "        tool_obj = tool(tool_obj)\n",
        "\n",
        "    @tool(tool_obj.name, description=tool_obj.description, args_schema=tool_obj.args_schema)\n",
        "    def wrapped(**tool_input):\n",
        "        request = [{\n",
        "            \"action_request\": {\"action\": tool_obj.name, \"args\": tool_input},\n",
        "            \"config\": {\"allow_accept\": True, \"allow_edit\": True, \"allow_respond\": True},\n",
        "            \"description\": \"Review this tool call\"\n",
        "        }]\n",
        "        response = interrupt(request)[0]\n",
        "        if response[\"type\"] == \"accept\":\n",
        "            return tool_obj.invoke(tool_input)\n",
        "        if response[\"type\"] == \"edit\":\n",
        "            new_args = response[\"args\"][\"args\"]\n",
        "            return tool_obj.invoke(new_args)\n",
        "        if response[\"type\"] == \"response\":\n",
        "            return response[\"args\"]\n",
        "        raise ValueError(\"Unsupported interrupt response type\")\n",
        "    return wrapped\n",
        "\n",
        "@tool(\"echo_search\")\n",
        "def echo_search(query: str) -> str:\n",
        "    \"\"\"Demo tool that simulates a search by echoing the query.\"\"\"\n",
        "    return f\"Search results for: {query}\"\n",
        "\n",
        "WRAPPED_SEARCH = add_hitl(echo_search)\n",
        "\n",
        "@task\n",
        "def e_call_model(messages: List[Dict[str, Any]]):\n",
        "    return LLM.bind_tools([WRAPPED_SEARCH]).invoke(messages)\n",
        "\n",
        "@task\n",
        "def e_call_tool(tool_call: Dict[str, Any]) -> ToolMessage:\n",
        "    obs = WRAPPED_SEARCH.invoke(tool_call[\"args\"])\n",
        "    return ToolMessage(content=obs, tool_call_id=tool_call[\"id\"])\n",
        "\n",
        "from langgraph.func import entrypoint as ep\n",
        "\n",
        "@ep(checkpointer=CHECKPOINTER)\n",
        "def e_agent(messages: List[Dict[str, Any]], previous: Optional[List[Dict[str, Any]]] = None):\n",
        "    if previous is not None:\n",
        "        messages = add_messages(previous, messages)\n",
        "    llm_msg = e_call_model(messages).result()\n",
        "    while True:\n",
        "        tcs = getattr(llm_msg, \"tool_calls\", None) or []\n",
        "        if not tcs:\n",
        "            break\n",
        "        tool_results = [e_call_tool(tc).result() for tc in tcs]\n",
        "        messages = add_messages(messages, [llm_msg, *tool_results])\n",
        "        llm_msg = e_call_model(messages).result()\n",
        "    messages = add_messages(messages, llm_msg)\n",
        "    return ep.final(value=llm_msg, save=messages)"
      ],
      "metadata": {
        "id": "4ih1QJQnRroN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pattern F: Static interrupts for debugging"
      ],
      "metadata": {
        "id": "ri4q8t4HRtLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_graph_F():\n",
        "    class S(TypedDict, total=False):\n",
        "        x: int\n",
        "\n",
        "    def a(state: S) -> S:\n",
        "        return {\"x\": 1}\n",
        "\n",
        "    def b(state: S) -> S:\n",
        "        return {\"x\": state[\"x\"] + 1}\n",
        "\n",
        "    g = StateGraph(S)\n",
        "    g.add_node(\"a\", a)\n",
        "    g.add_node(\"b\", b)\n",
        "    g.set_entry_point(\"a\")\n",
        "    g.add_edge(\"a\", \"b\")\n",
        "    g.add_edge(\"b\", END)\n",
        "    return g.compile(\n",
        "        checkpointer=CHECKPOINTER,\n",
        "        interrupt_before=[\"a\"],\n",
        "        interrupt_after=[\"b\"]\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "FSKUV8cJRvYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interactive runner"
      ],
      "metadata": {
        "id": "eUc2NGNvRxwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wait_for_interrupt_and_prompt(app, cfg):\n",
        "    \"\"\"\n",
        "    Drive interrupts from the terminal.\n",
        "    Supports single payloads and lists used by wrapped tools.\n",
        "    Also supports parallel interrupts by auto building a resume map.\n",
        "    \"\"\"\n",
        "    state = app.get_state(cfg)\n",
        "    ints = getattr(state, \"interrupts\", []) or []\n",
        "    if not ints:\n",
        "        print(\"No interrupts pending\")\n",
        "        return None\n",
        "\n",
        "    if len(ints) > 1:\n",
        "        print(\"\\nMultiple interrupts pending:\")\n",
        "        for i, it in enumerate(ints, 1):\n",
        "            print(f\"[{i}] id={it.interrupt_id} value={jdump(it.value)}\")\n",
        "        print(\"Enter values per interrupt. Leave blank to echo original.\")\n",
        "        resume_map = {}\n",
        "        for it in ints:\n",
        "            val = input(f\"Value for {it.interrupt_id}: \").strip()\n",
        "            if val:\n",
        "                # Try JSON, else raw string\n",
        "                try:\n",
        "                    resume_map[it.interrupt_id] = json.loads(val)\n",
        "                except Exception:\n",
        "                    resume_map[it.interrupt_id] = val\n",
        "            else:\n",
        "                resume_map[it.interrupt_id] = it.value\n",
        "        return Command(resume=resume_map)\n",
        "\n",
        "    # Single interrupt path\n",
        "    it = ints[0]\n",
        "    print(\"\\nInterrupt payload:\")\n",
        "    print(jdump(it.value))\n",
        "    print(\"Enter resume value. For tool wrapper use examples like\")\n",
        "    print(\"  {\\\"type\\\": \\\"accept\\\"}\")\n",
        "    print(\"  {\\\"type\\\": \\\"edit\\\", \\\"args\\\": {\\\"args\\\": {\\\"query\\\": \\\"weather in NY\\\"}}}\")\n",
        "    print(\"  {\\\"type\\\": \\\"response\\\", \\\"args\\\": \\\"Skip tool right now\\\"}\")\n",
        "    raw = input(\"resume> \").strip()\n",
        "    if not raw:\n",
        "        val = it.value\n",
        "    else:\n",
        "        try:\n",
        "            val = json.loads(raw)\n",
        "        except Exception:\n",
        "            val = raw\n",
        "    return Command(resume=val)\n",
        "\n",
        "def run_pattern_A():\n",
        "    app = build_graph_A()\n",
        "    cfg = {\"configurable\": {\"thread_id\": f\"A-{uuid.uuid4()}\"}}\n",
        "    topic = input(\"Enter LinkedIn topic: \").strip() or \"Human in the loop for agents\"\n",
        "    stream = app.stream({\"linkedin_topic\": topic, \"human_feedback\": []}, config=cfg)\n",
        "    while True:\n",
        "        try:\n",
        "            step = next(stream)\n",
        "        except StopIteration:\n",
        "            break\n",
        "        if \"__interrupt__\" in step:\n",
        "            cmd = wait_for_interrupt_and_prompt(app, cfg)\n",
        "            stream = app.stream(cmd, cfg)\n",
        "    print(\"Done A\")\n",
        "\n",
        "def run_pattern_B():\n",
        "    app = build_graph_B()\n",
        "    cfg = {\"configurable\": {\"thread_id\": f\"B-{uuid.uuid4()}\"}}\n",
        "    _ = app.invoke({}, config=cfg)\n",
        "    while True:\n",
        "        cmd = wait_for_interrupt_and_prompt(app, cfg)\n",
        "        _ = app.invoke(cmd, config=cfg)\n",
        "        state = app.get_state(cfg)\n",
        "        if not state.interrupts:\n",
        "            print(\"Final state:\", state.values)\n",
        "            break\n",
        "    print(\"Done B\")\n",
        "\n",
        "def run_pattern_C():\n",
        "    app = build_graph_C()\n",
        "    cfg = {\"configurable\": {\"thread_id\": f\"C-{uuid.uuid4()}\"}}\n",
        "    _ = app.invoke({}, config=cfg)\n",
        "    cmd = wait_for_interrupt_and_prompt(app, cfg)\n",
        "    final = app.invoke(cmd, config=cfg)\n",
        "    print(\"Final C:\", final)\n",
        "\n",
        "def run_pattern_D():\n",
        "    app = build_graph_D()\n",
        "    cfg = {\"configurable\": {\"thread_id\": f\"D-{uuid.uuid4()}\"}}\n",
        "    _ = app.invoke({\"text_1\": \"alpha\", \"text_2\": \"beta\"}, config=cfg)\n",
        "    cmd = wait_for_interrupt_and_prompt(app, cfg)\n",
        "    final = app.invoke(cmd, config=cfg)\n",
        "    print(\"Final D:\", final)\n",
        "\n",
        "def run_pattern_E():\n",
        "    cfg = {\"configurable\": {\"thread_id\": f\"E-{uuid.uuid4()}\"}}\n",
        "    user_msg = {\"role\": \"user\", \"content\": \"Search for current weather in San Francisco\"}\n",
        "    stream = e_agent.stream([user_msg], cfg)\n",
        "    while True:\n",
        "        try:\n",
        "            step = next(stream)\n",
        "        except StopIteration:\n",
        "            break\n",
        "        if \"__interrupt__\" in step:\n",
        "            cmd = wait_for_interrupt_and_prompt(e_agent, cfg)\n",
        "            stream = e_agent.stream(cmd, cfg)\n",
        "        else:\n",
        "            print(step)\n",
        "    print(\"Done E\")\n",
        "\n",
        "def run_pattern_F():\n",
        "    app = build_graph_F()\n",
        "    cfg = {\"configurable\": {\"thread_id\": f\"F-{uuid.uuid4()}\"}}\n",
        "    _ = app.invoke({}, config=cfg)\n",
        "    print(\"Breakpoint before a recorded. Resuming\")\n",
        "    _ = app.invoke(None, config=cfg)\n",
        "    print(\"Breakpoint after b recorded. Resuming\")\n",
        "    final = app.invoke(None, config=cfg)\n",
        "    print(\"Final F:\", final)\n",
        "\n",
        "def main():\n",
        "    menu = \"\"\"\n",
        "Pick a demo\n",
        "1. Human feedback loop for writing\n",
        "2. Approval gate before API call\n",
        "3. Review and edit state\n",
        "4. Parallel interrupts with resume map\n",
        "5. Tool call review in a tiny ReAct loop\n",
        "6. Static interrupts for debugging\n",
        "q. Quit\n",
        "> \"\"\"\n",
        "    while True:\n",
        "        choice = input(menu).strip().lower()\n",
        "        if choice == \"1\":\n",
        "            run_pattern_A()\n",
        "        elif choice == \"2\":\n",
        "            run_pattern_B()\n",
        "        elif choice == \"3\":\n",
        "            run_pattern_C()\n",
        "        elif choice == \"4\":\n",
        "            run_pattern_D()\n",
        "        elif choice == \"5\":\n",
        "            run_pattern_E()\n",
        "        elif choice == \"6\":\n",
        "            run_pattern_F()\n",
        "        elif choice in {\"q\", \"quit\", \"exit\"}:\n",
        "            sys.exit(0)\n",
        "        else:\n",
        "            print(\"Unknown choice\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e7RnB-3hEMlM",
        "outputId": "f4c3a3d5-34bd-4a20-cacb-23700aea1819"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Pick a demo\n",
            "1. Human feedback loop for writing\n",
            "2. Approval gate before API call\n",
            "3. Review and edit state\n",
            "4. Parallel interrupts with resume map\n",
            "5. Tool call review in a tiny ReAct loop\n",
            "6. Static interrupts for debugging\n",
            "q. Quit\n",
            "> 1\n",
            "Enter LinkedIn topic: Ai agents\n",
            "\n",
            "[model] Draft:\n",
            "\"As we continue to push the boundaries of innovation, AI agents are revolutionizing the way we work and interact. From virtual assistants to intelligent chatbots, these agents are streamlining processes, enhancing customer experiences, and unlocking new possibilities. What are your thoughts on the future of AI agents? How do you see them transforming your industry? Let's start the conversation! #AI #ArtificialIntelligence #Innovation\"\n",
            "\n",
            "\n",
            "[human] awaiting feedback. Type done to finish\n",
            "\n",
            "Interrupt payload:\n",
            "{\n",
            "  \"generated_post\": \"\\\"As we continue to push the boundaries of innovation, AI agents are revolutionizing the way we work and interact. From virtual assistants to intelligent chatbots, these agents are streamlining processes, enhancing customer experiences, and unlocking new possibilities. What are your thoughts on the future of AI agents? How do you see them transforming your industry? Let's start the conversation! #AI #ArtificialIntelligence #Innovation\\\"\",\n",
            "  \"message\": \"Provide feedback or type done\"\n",
            "}\n",
            "Enter resume value. For tool wrapper use examples like\n",
            "  {\"type\": \"accept\"}\n",
            "  {\"type\": \"edit\", \"args\": {\"args\": {\"query\": \"weather in NY\"}}}\n",
            "  {\"type\": \"response\", \"args\": \"Skip tool right now\"}\n",
            "resume> accept\n",
            "\n",
            "[human] awaiting feedback. Type done to finish\n",
            "[human] feedback: accept\n",
            "\n",
            "[model] Draft:\n",
            "\"As AI agents continue to evolve, they're revolutionizing industries and transforming the way we work. With their ability to learn, adapt, and make decisions, they're becoming an integral part of our professional lives. What are your thoughts on the future of AI agents in the workplace? How do you see them impacting your industry? Let's discuss! #AI #ArtificialIntelligence #FutureOfWork\"\n",
            "\n",
            "\n",
            "[human] awaiting feedback. Type done to finish\n",
            "\n",
            "Interrupt payload:\n",
            "{\n",
            "  \"generated_post\": \"\\\"As AI agents continue to evolve, they're revolutionizing industries and transforming the way we work. With their ability to learn, adapt, and make decisions, they're becoming an integral part of our professional lives. What are your thoughts on the future of AI agents in the workplace? How do you see them impacting your industry? Let's discuss! #AI #ArtificialIntelligence #FutureOfWork\\\"\",\n",
            "  \"message\": \"Provide feedback or type done\"\n",
            "}\n",
            "Enter resume value. For tool wrapper use examples like\n",
            "  {\"type\": \"accept\"}\n",
            "  {\"type\": \"edit\", \"args\": {\"args\": {\"query\": \"weather in NY\"}}}\n",
            "  {\"type\": \"response\", \"args\": \"Skip tool right now\"}\n",
            "resume> edit\n",
            "\n",
            "[human] awaiting feedback. Type done to finish\n",
            "[human] feedback: edit\n",
            "\n",
            "[model] Draft:\n",
            "As AI agents continue to evolve, they're transforming industries and revolutionizing the way we work. With their ability to learn, adapt, and make decisions, AI agents are becoming increasingly integral to business operations. What are your thoughts on the future of AI agents in the workplace? Let's discuss! #AI #ArtificialIntelligence #FutureOfWork\n",
            "\n",
            "\n",
            "[human] awaiting feedback. Type done to finish\n",
            "\n",
            "Interrupt payload:\n",
            "{\n",
            "  \"generated_post\": \"As AI agents continue to evolve, they're transforming industries and revolutionizing the way we work. With their ability to learn, adapt, and make decisions, AI agents are becoming increasingly integral to business operations. What are your thoughts on the future of AI agents in the workplace? Let's discuss! #AI #ArtificialIntelligence #FutureOfWork\",\n",
            "  \"message\": \"Provide feedback or type done\"\n",
            "}\n",
            "Enter resume value. For tool wrapper use examples like\n",
            "  {\"type\": \"accept\"}\n",
            "  {\"type\": \"edit\", \"args\": {\"args\": {\"query\": \"weather in NY\"}}}\n",
            "  {\"type\": \"response\", \"args\": \"Skip tool right now\"}\n",
            "resume> accept\n",
            "\n",
            "[human] awaiting feedback. Type done to finish\n",
            "[human] feedback: accept\n",
            "\n",
            "[model] Draft:\n",
            "\"As AI agents continue to evolve, they're revolutionizing industries and transforming the way we work. With their ability to learn, adapt, and make decisions, they're becoming an integral part of our professional lives. What are your thoughts on the future of AI agents in the workplace? How do you see them impacting your industry? Let's discuss! #AI #ArtificialIntelligence #FutureOfWork\"\n",
            "\n",
            "\n",
            "[human] awaiting feedback. Type done to finish\n",
            "\n",
            "Interrupt payload:\n",
            "{\n",
            "  \"generated_post\": \"\\\"As AI agents continue to evolve, they're revolutionizing industries and transforming the way we work. With their ability to learn, adapt, and make decisions, they're becoming an integral part of our professional lives. What are your thoughts on the future of AI agents in the workplace? How do you see them impacting your industry? Let's discuss! #AI #ArtificialIntelligence #FutureOfWork\\\"\",\n",
            "  \"message\": \"Provide feedback or type done\"\n",
            "}\n",
            "Enter resume value. For tool wrapper use examples like\n",
            "  {\"type\": \"accept\"}\n",
            "  {\"type\": \"edit\", \"args\": {\"args\": {\"query\": \"weather in NY\"}}}\n",
            "  {\"type\": \"response\", \"args\": \"Skip tool right now\"}\n",
            "resume> done\n",
            "\n",
            "[human] awaiting feedback. Type done to finish\n",
            "[human] feedback: done\n",
            "\n",
            "[end] Final post:\n",
            "\"As AI agents continue to evolve, they're revolutionizing industries and transforming the way we work. With their ability to learn, adapt, and make decisions, they're becoming an integral part of our professional lives. What are your thoughts on the future of AI agents in the workplace? How do you see them impacting your industry? Let's discuss! #AI #ArtificialIntelligence #FutureOfWork\"\n",
            "[end] Feedback trail: ['accept', 'edit', 'accept', 'Finalised']\n",
            "Done A\n",
            "\n",
            "Pick a demo\n",
            "1. Human feedback loop for writing\n",
            "2. Approval gate before API call\n",
            "3. Review and edit state\n",
            "4. Parallel interrupts with resume map\n",
            "5. Tool call review in a tiny ReAct loop\n",
            "6. Static interrupts for debugging\n",
            "q. Quit\n",
            "> q\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "0",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NHF7s9QxJLWO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}