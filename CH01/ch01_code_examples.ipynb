{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Open in Colab\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/Nicolepcx/ai-agents-the-definitive-guide/blob/main/CH01/ch01_code_examples.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "Eai0eqmW7v1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About this notebook\n",
        "\n",
        "This notebook is a compact, end to end primer on turning a plain LLM call into a small but real agent that can use tools, keep state across turns, and trace its own reasoning flow.\n",
        "\n",
        "## What it shows\n",
        "\n",
        "* **Stateless LLM call** with `langchain-openai` to warm up.\n",
        "* **Tool use in a loop** with two simple tools\n",
        "\n",
        "  1. `internet_search` via SerpAPI for fresh information\n",
        "  2. `calculator` powered by `numexpr` for fast math.\n",
        "* **A minimal LangGraph agent** with\n",
        "\n",
        "  * a typed state that accumulates `messages` using `add_messages`\n",
        "  * an `llm` node bound to tools and a `ToolNode` for execution\n",
        "  * a router that decides when to call tools or stop\n",
        "  * in memory checkpoints via `MemorySaver`\n",
        "  * separate threads to branch conversations and compare memory.\n",
        "\n",
        "## Why these examples\n",
        "\n",
        "1. You see the **bare minimum loop** that handles tool calls without any framework state.\n",
        "2. You then see the **same idea done right** with LangGraph, so you get clean routing, checkpoints, and thread scoped memory.\n",
        "3. You get **trace utilities** to print node updates and a short state snapshot, which makes debugging and teaching much easier.\n",
        "\n",
        "## How to run it\n",
        "\n",
        "* Load keys from `.env` or set them with `%env`. If missing, the notebook will ask interactively.\n",
        "* Call `gpt-5-mini` or any other LLM you like compatible with OAI API, once for a quick check, then switch to `gpt-4o` bound to tools.\n",
        "* Run a **two step task**:\n",
        "  * get the **current air temperature in New York City** using `internet_search`\n",
        "  * **square the temperature** with `calculator`.\n",
        "* Repeat the task inside a **LangGraph** app, then branch a **second thread** that converts the temperature to Fahrenheit and reports both.\n",
        "  You will see per node updates and a final memory snapshot for each thread.\n",
        "\n",
        "## Key ideas to notice\n",
        "\n",
        "* **Binding tools to the model** and letting the model choose when to call them.\n",
        "* A **minimal router** that checks for `tool_calls` and either transitions to the `ToolNode` or ends.\n",
        "* **Thread ids** for independent memories and reproducible debugging.\n",
        "* **Deterministic setups** where helpful\n",
        "  `temperature=0`, explicit `recursion_limit`, and short format prompts to produce consistent outputs.\n",
        "\n",
        "## Swap and extend\n",
        "\n",
        "* You can swap `SerpAPIWrapper` with any other search tool that returns text.\n",
        "* Add your own tools and drop them into `tools` and `tool_map`.\n",
        "* Replace `MemorySaver` with a persistent checkpointer if you want long lived sessions.\n",
        "* Add nodes for planning, validation, or guardrails if you want a larger graph.\n",
        "\n",
        "## Requirements and notes\n",
        "\n",
        "* You need a working [`OPENAI_API_KEY`](https://platform.openai.com/api-keys) and a [`SerpAPIWrapper`](https://serpapi.com/).\n",
        "* Internet search results change. The reported temperature and snippets will vary by time and source.\n",
        "* Tool calls count toward tokens and external API usage. Keep an eye on cost.\n"
      ],
      "metadata": {
        "id": "o_Wb2z1-N7Mu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "t-p_wQKJl5io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langgraph==0.6.7 langchain-openai==0.3.33 python-dotenv==1.1.1 langchain_community google-search-results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABZWkn2AASy1",
        "outputId": "a169792f-4af4-4308-e6da-183d8592a4af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports for API"
      ],
      "metadata": {
        "id": "-gx3m_2cl8b2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from dotenv import load_dotenv\n",
        "import os"
      ],
      "metadata": {
        "id": "IO_MMShOBGRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "bnfEHkKAl9v6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library\n",
        "import os\n",
        "import math\n",
        "import json\n",
        "import numexpr\n",
        "from typing import List, Dict, Any, TypedDict, Annotated\n",
        "\n",
        "# LangChain core\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage\n",
        "from langchain_core.tools import tool\n",
        "from langchain_community.utilities import SerpAPIWrapper\n",
        "\n",
        "# LangGraph\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langgraph.checkpoint.memory import MemorySaver\n"
      ],
      "metadata": {
        "id": "yXst6GDwPSog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- API Key Setup ---\n",
        "# Option 1 (preferred): create a `.env` file in your project folder with:\n",
        "# OPENAI_API_KEY=your_openai_key_here\n",
        "# SERPAPI_API_KEY=your_serpapi_key_here\n",
        "#\n",
        "# Option 2: set it directly in the notebook with magic:\n",
        "# %env OPENAI_API_KEY=your_openai_key_here\n",
        "# %env SERPAPI_API_KEY=your_serpapi_key_here\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load from .env if available\n",
        "load_dotenv()\n",
        "\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "serp_api_key = os.getenv(\"SERPAPI_API_KEY\")\n",
        "\n",
        "# Fallback: ask if still missing\n",
        "if not OPENAI_API_KEY:\n",
        "    print(\"⚠️ OPENAI_API_KEY not found. You can set it with `%env` in the notebook or enter it below.\")\n",
        "    OPENAI_API_KEY = input(\"Enter your OPENAI_API_KEY: \").strip()\n",
        "\n",
        "if not serp_api_key:\n",
        "    print(\"⚠️ SERPAPI_API_KEY not found. You can set it with `%env` in the notebook or enter it below.\")\n",
        "    serp_api_key = input(\"Enter your SERPAPI_API_KEY: \").strip()\n",
        "\n",
        "print(\"✅ API keys loaded successfully!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "tMGB6TAqAVwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Example: Stateless LLM call with LangChain\n",
        "____"
      ],
      "metadata": {
        "id": "wzvqqoqqRlP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup LLM"
      ],
      "metadata": {
        "id": "X2G-2H5tQU6x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB33wpPzAErJ",
        "outputId": "9d48c20e-5423-4fe8-f366-b0b8829208ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Short answer\n",
            "An AI agent is a software (or embodied) system that perceives its environment, makes decisions, and takes actions to achieve goals — usually with some degree of autonomy and adaptability.\n",
            "\n",
            "Key characteristics\n",
            "- Perception: senses inputs (camera, mic, sensors, API data, user text).\n",
            "- Decision-making: chooses actions based on goals, models, or learned policies.\n",
            "- Action: affects the environment (move a robot, send a message, place a trade).\n",
            "- Autonomy: operates without requiring step-by-step human control.\n",
            "- Goal-directedness: usually tries to maximize a reward or satisfy objectives.\n",
            "\n",
            "Common types and examples\n",
            "- Reactive agents: map inputs directly to actions (simple controllers, thermostats).\n",
            "- Deliberative/planning agents: build internal models and plan ahead (robot path planners).\n",
            "- Learning agents: improve with experience (reinforcement learning agents, recommendation systems).\n",
            "- Hybrid agents: combine rules, planning, and learning.\n",
            "- Embodied agents: robots, autonomous vehicles.\n",
            "- Software agents: chatbots, virtual assistants, web crawlers, trading bots.\n",
            "- Multi-agent systems: multiple agents interacting or negotiating (simulation, swarm robotics).\n",
            "\n",
            "Typical components\n",
            "- Sensors/input layer (perception)\n",
            "- World model or memory (state, beliefs)\n",
            "- Decision module (rules, planner, policy, or LLM)\n",
            "- Action/output layer (actuators, APIs)\n",
            "- Learning/updating mechanism (optional)\n",
            "\n",
            "How they’re built\n",
            "- Rule-based systems: explicit rules and logic.\n",
            "- Model-based planners: search and optimization over possible actions.\n",
            "- Machine learning: supervised or reinforcement learning to derive policies.\n",
            "- LLM-based agents: language models orchestrate tools, planning, and memory for complex tasks.\n",
            "\n",
            "Applications\n",
            "- Personal assistants (scheduling, email triage)\n",
            "- Autonomous vehicles and drones\n",
            "- Industrial automation and robotics\n",
            "- Customer support chatbots\n",
            "- Finance: algorithmic trading and portfolio management\n",
            "- Healthcare diagnostics assistance\n",
            "- Simulation and gaming (NPCs, training sims)\n",
            "\n",
            "Limitations and risks\n",
            "- Brittleness outside training or design conditions\n",
            "- Erroneous or biased outputs from bad data or objectives\n",
            "- Safety hazards for physical agents (collisions, damage)\n",
            "- Security and adversarial manipulation\n",
            "- Alignment and ethical concerns for goal-setting and autonomy\n",
            "\n",
            "Good practices\n",
            "- Define clear objectives and constraints\n",
            "- Test extensively in realistic scenarios and edge cases\n",
            "- Provide human oversight and fail-safe controls\n",
            "- Monitor, log, and update models based on performance\n",
            "- Consider transparency, privacy, and ethical impacts\n",
            "\n",
            "If you want, I can:\n",
            "- Give specific examples (e.g., how a Roomba vs. ChatGPT-like agent is built).\n",
            "- Explain architectures (reactive vs. deliberative vs. memory-based).\n",
            "- Suggest tools and libraries for building agents (RL frameworks, agent platforms).\n"
          ]
        }
      ],
      "source": [
        "llm = ChatOpenAI(model=\"gpt-5-mini\")\n",
        "response = llm.invoke(\"What are AI agents?\")\n",
        "print(response.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qLlXC4_8E38-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining tools for a stateless run\n"
      ],
      "metadata": {
        "id": "6KoEJWquRuPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools"
      ],
      "metadata": {
        "id": "5wc9WUocQbTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tool(\"internet_search\")\n",
        "def internet_search(query: str) -> str:\n",
        "    \"\"\"Search Google via SerpAPI for up to date information.\"\"\"\n",
        "    serp_api_key = os.environ[\"SERPAPI_API_KEY\"]\n",
        "    params = {\"engine\": \"google\", \"gl\": \"us\", \"hl\": \"en\"}\n",
        "    search = SerpAPIWrapper(params=params, serpapi_api_key=serp_api_key)\n",
        "    return search.run(query)\n",
        "\n",
        "@tool(\"calculator\")\n",
        "def calculator(expression: str) -> str:\n",
        "    \"\"\"Evaluate a single line mathematical expression with numexpr.\"\"\"\n",
        "    local_dict = {\"pi\": math.pi, \"e\": math.e}\n",
        "    out = numexpr.evaluate(\n",
        "        expression.strip(),\n",
        "        global_dict={},\n",
        "        local_dict=local_dict,\n",
        "    )\n",
        "    return str(out)\n",
        "\n",
        "tools = [internet_search, calculator]\n",
        "\n",
        "tool_map: Dict[str, Any] = {t.name: t for t in tools}"
      ],
      "metadata": {
        "id": "t4T69i17Qflt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model bound to tool"
      ],
      "metadata": {
        "id": "MOXl8bCwQlG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o\").bind_tools(tools, tool_choice=\"any\")"
      ],
      "metadata": {
        "id": "945dewpDQtxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stateless single run with tool loop"
      ],
      "metadata": {
        "id": "TZnTP-2bqDQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_once(prompt: str, max_steps: int = 4) -> str:\n",
        "    messages = [HumanMessage(content=prompt)]\n",
        "    for _ in range(max_steps):\n",
        "        ai: AIMessage = llm.invoke(messages)\n",
        "        messages.append(ai)\n",
        "\n",
        "        calls = getattr(ai, \"tool_calls\", None) or []\n",
        "        if not calls:\n",
        "            break\n",
        "\n",
        "        for call in calls:\n",
        "            name = call[\"name\"]\n",
        "            args = call.get(\"args\", {})\n",
        "            result = tool_map[name].invoke(args)\n",
        "            messages.append(ToolMessage(\n",
        "                content=str(result),\n",
        "                name=name,\n",
        "                tool_call_id=call[\"id\"]\n",
        "            ))\n",
        "    return messages[-1].content\n",
        "\n",
        "print(run_once(\"\"\"Two step task.\n",
        "\n",
        "Step 1: Use internet_search to get the current air temperature in New York City today. Show the exact query you used, the top source title and snippet, and extract a numeric temperature in Celsius. Return this temperature as feedback for Step 2.\n",
        "\n",
        "Step 2: Using the Celsius value from Step 1, compute its square with calculator. Show the exact expression you used and the numeric result.\n",
        "\n",
        "Important: Give a short final answer in this format:\n",
        "Current temperature:\n",
        "Square of current temperature:\"\"\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkrNl0WkEXdb",
        "outputId": "6aaccf14-dfe2-46f6-9dc8-c3fc8acd75b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tools"
      ],
      "metadata": {
        "id": "f9qrsmfpQzAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tool(\"internet_search\")\n",
        "def internet_search(query: str) -> str:\n",
        "    \"\"\"Search Google via SerpAPI for up to date information.\"\"\"\n",
        "    serp_api_key = os.environ[\"SERPAPI_API_KEY\"]\n",
        "    params = {\"engine\": \"google\", \"gl\": \"us\", \"hl\": \"en\"}\n",
        "    search = SerpAPIWrapper(params=params, serpapi_api_key=serp_api_key)\n",
        "    return search.run(query)\n",
        "\n",
        "@tool(\"calculator\")\n",
        "def calculator(expression: str) -> str:\n",
        "    \"\"\"Evaluate a single line mathematical expression with numexpr.\"\"\"\n",
        "    local_dict = {\"pi\": math.pi, \"e\": math.e}\n",
        "    out = numexpr.evaluate(\n",
        "        expression.strip(),\n",
        "        global_dict={},\n",
        "        local_dict=local_dict,\n",
        "    )\n",
        "    return str(out)\n",
        "\n",
        "tools = [internet_search, calculator]\n",
        "tool_map: Dict[str, Any] = {t.name: t for t in tools}\n"
      ],
      "metadata": {
        "id": "0Z0Dr49JQ-R-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model bound to tools"
      ],
      "metadata": {
        "id": "JUD84yXhQ_sz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, max_tokens=800).bind_tools(tools, tool_choice=\"auto\")"
      ],
      "metadata": {
        "id": "CLh51gxsREvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Minimal tool loop (stateless)"
      ],
      "metadata": {
        "id": "iHt5l32vRLfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def run_once(prompt: str, max_steps: int = 8) -> str:\n",
        "    messages: List[HumanMessage | AIMessage | ToolMessage] = [HumanMessage(content=prompt)]\n",
        "    last_ai: AIMessage | None = None\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "        ai: AIMessage = llm.invoke(messages)\n",
        "        messages.append(ai)\n",
        "        last_ai = ai\n",
        "\n",
        "        calls = getattr(ai, \"tool_calls\", None) or []\n",
        "        if not calls:\n",
        "            # Model produced a final answer\n",
        "            return messages[-1].content\n",
        "\n",
        "        # Execute tool calls and feed observations back\n",
        "        for call in calls:\n",
        "            name = call[\"name\"]\n",
        "            args = call.get(\"args\", {}) or {}\n",
        "            result = tool_map[name].invoke(args)\n",
        "            messages.append(ToolMessage(\n",
        "                content=str(result),\n",
        "                name=name,\n",
        "                tool_call_id=call.get(\"id\")\n",
        "            ))\n",
        "\n",
        "    # If we exit the loop without a clean final AI message, force a wrap up\n",
        "    messages.append(HumanMessage(content=\"\"\"\n",
        "Finish now. Give a short final answer in this exact format:\n",
        "\n",
        "Current temperature:\n",
        "Square of current temperature:\n",
        "\"\"\".strip()))\n",
        "    final_ai: AIMessage = llm.invoke(messages)\n",
        "    return final_ai.content\n",
        "\n",
        "print(run_once(\"\"\"Two step task.\n",
        "\n",
        "Step 1: Use internet_search to get the current air temperature in New York City today. Show the exact query you used, the top source title and snippet, and extract a numeric temperature in Celsius. Return this temperature as feedback for Step 2.\n",
        "\n",
        "Step 2: Using the Celsius value from Step 1, compute its square with calculator. Show the exact expression you used and the numeric result.\n",
        "\n",
        "Important: Give a short final answer in this format:\n",
        "Current temperature:\n",
        "Square of current temperature:\"\"\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FTNsLGbETmt",
        "outputId": "bf13c6fb-c7a5-4895-9437-9d364442655b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current temperature: 16°C\n",
            "Square of current temperature: 256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools"
      ],
      "metadata": {
        "id": "d76y1r4tRfqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tool(\"internet_search\")\n",
        "def internet_search(query: str) -> str:\n",
        "    \"\"\"Search Google via SerpAPI for up to date information.\"\"\"\n",
        "    serp_api_key = os.environ[\"SERPAPI_API_KEY\"]\n",
        "    params = {\"engine\": \"google\", \"gl\": \"us\", \"hl\": \"en\"}\n",
        "    search = SerpAPIWrapper(params=params, serpapi_api_key=serp_api_key)\n",
        "    return search.run(query)\n",
        "\n",
        "@tool(\"calculator\")\n",
        "def calculator(expression: str) -> str:\n",
        "    \"\"\"Evaluate a single line mathematical expression with numexpr.\"\"\"\n",
        "    local_dict = {\"pi\": math.pi, \"e\": math.e}\n",
        "    out = numexpr.evaluate(\n",
        "        expression.strip(),\n",
        "        global_dict={},\n",
        "        local_dict=local_dict,\n",
        "    )\n",
        "    return str(out)\n",
        "\n",
        "tools = [internet_search, calculator]\n"
      ],
      "metadata": {
        "id": "iVf9KPbwqdKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a minimal LangGraph with state, nodes, and routing"
      ],
      "metadata": {
        "id": "EdYgKmmRqgme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[List[BaseMessage], add_messages]\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, max_tokens=800).bind_tools(tools)\n",
        "\n",
        "def llm_node(state: AgentState) -> AgentState:\n",
        "    ai = llm.invoke(state[\"messages\"])\n",
        "    return {\"messages\": [ai]}\n",
        "\n",
        "tool_node = ToolNode(tools=tools)\n",
        "\n",
        "graph = StateGraph(AgentState)\n",
        "graph.add_node(\"llm\", llm_node)\n",
        "graph.add_node(\"tools\", tool_node)\n",
        "graph.add_edge(START, \"llm\")\n",
        "\n",
        "def route(state: AgentState):\n",
        "    last = state[\"messages\"][-1]\n",
        "    calls = getattr(last, \"tool_calls\", None) or []\n",
        "    return \"tools\" if calls else END\n",
        "\n",
        "graph.add_conditional_edges(\"llm\", route, {\"tools\": \"tools\", END: END})\n",
        "graph.add_edge(\"tools\", \"llm\")"
      ],
      "metadata": {
        "id": "N9-_NR59qhs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compile with in-memory checkpoints and configure a thread"
      ],
      "metadata": {
        "id": "9RTBZ7q2qt7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpointer = MemorySaver()\n",
        "app = graph.compile(checkpointer=checkpointer)\n"
      ],
      "metadata": {
        "id": "hpM1rD80qx27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trace execution and inspect state and memory"
      ],
      "metadata": {
        "id": "I8bHWPPqq0xn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _short(msg: BaseMessage, max_len: int = 140) -> str:\n",
        "    \"\"\"Compact one-line view of a message.\"\"\"\n",
        "    role = type(msg).__name__.replace(\"Message\", \"\").lower()\n",
        "    content = getattr(msg, \"content\", \"\")\n",
        "    if isinstance(content, list):\n",
        "        # some tool outputs can be list payloads\n",
        "        try:\n",
        "            content = json.dumps(content)\n",
        "        except Exception:\n",
        "            content = str(content)\n",
        "    text = str(content).replace(\"\\n\", \" \").strip()\n",
        "    if len(text) > max_len:\n",
        "        text = text[: max_len - 3] + \"...\"\n",
        "    # include tool name or function call info when available\n",
        "    if hasattr(msg, \"tool_calls\") and getattr(msg, \"tool_calls\"):\n",
        "        tnames = [tc.get(\"name\", \"tool\") for tc in msg.tool_calls]\n",
        "        return f\"{role}: tool_calls -> {tnames}\"\n",
        "    if isinstance(msg, ToolMessage):\n",
        "        return f\"{role}({msg.name}): {text}\"\n",
        "    return f\"{role}: {text}\""
      ],
      "metadata": {
        "id": "y_3UaDQqq3n5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trace execution and inspect state and memory"
      ],
      "metadata": {
        "id": "RLbYfnFwq8sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_state_snapshot(app, config, title: str):\n",
        "    \"\"\"Print current graph state and memory for a given thread.\"\"\"\n",
        "    snap = app.get_state(config)\n",
        "    values = snap.values or {}\n",
        "    msgs: List[BaseMessage] = values.get(\"messages\", [])\n",
        "    print(f\"\\n=== {title} | state snapshot ===\")\n",
        "    print(f\"messages: {len(msgs)} total\")\n",
        "    for i, m in enumerate(msgs[-5:], start=max(0, len(msgs)-5) + 1):\n",
        "        print(f\"  {i:>3}: {_short(m)}\")\n",
        "    # show routing info and queued tasks if present\n",
        "    nxt = getattr(snap, \"next\", None)\n",
        "    tasks = getattr(snap, \"tasks\", None)\n",
        "    if nxt:\n",
        "        print(f\"next nodes: {list(nxt)}\")\n",
        "    if tasks:\n",
        "        print(f\"queued tasks: {tasks}\")\n",
        "    # minimal memory view via checkpointer for this thread\n",
        "    # MemorySaver keeps one latest checkpoint per thread by default, so show existence\n",
        "    print(\"memory: in-memory checkpoint present for this thread\")"
      ],
      "metadata": {
        "id": "rY4kuL8Wq_Pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trace execution and inspect state and memory"
      ],
      "metadata": {
        "id": "MfQyeqo7Ksc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_with_tracing(app, input_state: AgentState, config, title: str):\n",
        "    \"\"\"Run the graph while printing per-node updates and final memory.\"\"\"\n",
        "    print(f\"\\n=== {title} | execution trace ===\")\n",
        "    final = None\n",
        "    # stream_mode=\"updates\" surfaces node-level updates\n",
        "    for event in app.stream(input_state, config=config, stream_mode=\"updates\"):\n",
        "        for node, upd in event.items():\n",
        "            # upd is a dict like {\"messages\": [<new msg>]} or tool results\n",
        "            keys = list(upd.keys())\n",
        "            print(f\"[enter {node}] updated: {keys}\")\n",
        "            # if messages updated, print the last one briefly\n",
        "            msgs = upd.get(\"messages\") or []\n",
        "            if msgs:\n",
        "                print(f\"  {_short(msgs[-1])}\")\n",
        "            print(f\"[leave {node}]\")\n",
        "            final = upd\n",
        "    # show final assistant message from app.get_state\n",
        "    print_state_snapshot(app, config, title=f\"{title} | after run\")\n",
        "    snap = app.get_state(config)\n",
        "    msgs = snap.values.get(\"messages\", [])\n",
        "    return msgs[-1].content if msgs else \"\""
      ],
      "metadata": {
        "id": "BT_XdkdXKxLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configs\n",
        "cfg = {\"configurable\": {\"thread_id\": \"nyc-weather-session\"}}"
      ],
      "metadata": {
        "id": "3Q9WRmBnK3t9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Turn 1: get the current NYC air temperature in Celsius"
      ],
      "metadata": {
        "id": "BhVKSzsjK5wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "turn1_answer = run_with_tracing(\n",
        "    app,\n",
        "    {\"messages\": [HumanMessage(content=\"Get the current air temperature in New York City in Celsius.\")]},\n",
        "    config={**cfg, \"recursion_limit\": 20},\n",
        "    title=\"TURN 1\",\n",
        ")\n",
        "print(\"\\nTURN 1 (final assistant):\\n\", turn1_answer)"
      ],
      "metadata": {
        "id": "rrt286z5K_g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Turn 2: square that temperature in the same thread"
      ],
      "metadata": {
        "id": "08aCwAxcLC2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "turn2_answer = run_with_tracing(\n",
        "    app,\n",
        "    {\"messages\": [HumanMessage(content=\"Now compute the square of that temperature.\")]},\n",
        "    config={**cfg, \"recursion_limit\": 20},\n",
        "    title=\"TURN 2\",\n",
        ")\n",
        "print(\"\\nTURN 2 (final assistant):\\n\", turn2_answer)\n"
      ],
      "metadata": {
        "id": "qTq1ET5PLD8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Branch a parallel thread for a different follow-up"
      ],
      "metadata": {
        "id": "uXOOfeagLHY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg_branch = {\"configurable\": {\"thread_id\": \"nyc-weather-session-branch\"}}\n",
        "branch_answer = run_with_tracing(\n",
        "    app,\n",
        "    {\"messages\": [HumanMessage(content=\"Instead of squaring, convert it to Fahrenheit and report both.\")]},\n",
        "    config=cfg_branch,\n",
        "    title=\"BRANCH\",\n",
        ")\n",
        "print(\"\\nBRANCH (final assistant):\\n\", branch_answer)\n"
      ],
      "metadata": {
        "id": "SezHHqz2LJ0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# show consolidated memory views for both threads\n",
        "print_state_snapshot(app, cfg, title=\"MAIN THREAD memory view\")\n",
        "print_state_snapshot(app, cfg_branch, title=\"BRANCH THREAD memory view\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZruAg3xW4AE",
        "outputId": "99b4b4f6-ab84-4bb0-8bb7-e43c70ab7f6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TURN 1 | execution trace ===\n",
            "[enter llm] updated: ['messages']\n",
            "  ai: tool_calls -> ['internet_search']\n",
            "[leave llm]\n",
            "[enter tools] updated: ['messages']\n",
            "  tool(internet_search): {'type': 'weather_result', 'temperature': '18', 'unit': 'Celsius', 'precipitation': '0%', 'humidity': '80%', 'wind': '23 km/h', 'location...\n",
            "[leave tools]\n",
            "[enter llm] updated: ['messages']\n",
            "  ai: The current air temperature in New York City is 18°C.\n",
            "[leave llm]\n",
            "\n",
            "=== TURN 1 | after run | state snapshot ===\n",
            "messages: 4 total\n",
            "    1: human: Get the current air temperature in New York City in Celsius.\n",
            "    2: ai: tool_calls -> ['internet_search']\n",
            "    3: tool(internet_search): {'type': 'weather_result', 'temperature': '18', 'unit': 'Celsius', 'precipitation': '0%', 'humidity': '80%', 'wind': '23 km/h', 'location...\n",
            "    4: ai: The current air temperature in New York City is 18°C.\n",
            "memory: in-memory checkpoint present for this thread\n",
            "\n",
            "TURN 1 (final assistant):\n",
            " The current air temperature in New York City is 18°C.\n",
            "\n",
            "=== TURN 2 | execution trace ===\n",
            "[enter llm] updated: ['messages']\n",
            "  ai: tool_calls -> ['calculator']\n",
            "[leave llm]\n",
            "[enter tools] updated: ['messages']\n",
            "  tool(calculator): 16\n",
            "[leave tools]\n",
            "[enter llm] updated: ['messages']\n",
            "  ai: The square of the temperature, 18°C, is 324.\n",
            "[leave llm]\n",
            "\n",
            "=== TURN 2 | after run | state snapshot ===\n",
            "messages: 8 total\n",
            "    4: ai: The current air temperature in New York City is 18°C.\n",
            "    5: human: Now compute the square of that temperature.\n",
            "    6: ai: tool_calls -> ['calculator']\n",
            "    7: tool(calculator): 16\n",
            "    8: ai: The square of the temperature, 18°C, is 324.\n",
            "memory: in-memory checkpoint present for this thread\n",
            "\n",
            "TURN 2 (final assistant):\n",
            " The square of the temperature, 18°C, is 324.\n",
            "\n",
            "=== BRANCH | execution trace ===\n",
            "[enter llm] updated: ['messages']\n",
            "  ai: Sure, I can help with that. Please provide the temperature in Celsius that you would like to convert to Fahrenheit.\n",
            "[leave llm]\n",
            "\n",
            "=== BRANCH | after run | state snapshot ===\n",
            "messages: 2 total\n",
            "    1: human: Instead of squaring, convert it to Fahrenheit and report both.\n",
            "    2: ai: Sure, I can help with that. Please provide the temperature in Celsius that you would like to convert to Fahrenheit.\n",
            "memory: in-memory checkpoint present for this thread\n",
            "\n",
            "BRANCH (final assistant):\n",
            " Sure, I can help with that. Please provide the temperature in Celsius that you would like to convert to Fahrenheit.\n",
            "\n",
            "=== MAIN THREAD memory view | state snapshot ===\n",
            "messages: 8 total\n",
            "    4: ai: The current air temperature in New York City is 18°C.\n",
            "    5: human: Now compute the square of that temperature.\n",
            "    6: ai: tool_calls -> ['calculator']\n",
            "    7: tool(calculator): 16\n",
            "    8: ai: The square of the temperature, 18°C, is 324.\n",
            "memory: in-memory checkpoint present for this thread\n",
            "\n",
            "=== BRANCH THREAD memory view | state snapshot ===\n",
            "messages: 2 total\n",
            "    1: human: Instead of squaring, convert it to Fahrenheit and report both.\n",
            "    2: ai: Sure, I can help with that. Please provide the temperature in Celsius that you would like to convert to Fahrenheit.\n",
            "memory: in-memory checkpoint present for this thread\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ACETJV-qJxSd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}